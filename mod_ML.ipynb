{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous suivrons le pipeline ci-dessous :"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pipeline.jpg\" alt=\"drawing\" width=\"1000\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'objectif de cette étape est de vérifier les données (essentiellement leur cohérence, et leur qualité)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le module pandas permet de manipuler les données\n",
    "import pandas as pd\n",
    "\n",
    "# Les Fonctions permettant le checking des données\n",
    "from check_data import data_info, stats_data, boxplot_data, features_distributions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistiques descriptives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les données sous forme de tuple\n",
    "dataset = pd.read_csv(\"train_set.csv\")\n",
    "\n",
    "# Afficher un apperçu des données\n",
    "display(dataset.head())\n",
    "data_info(dataset)\n",
    "stats_data(dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons trois classes 0, 1, 2. Chaque classe représente une epsèce d'Iris, les codes sont les suivant :  \n",
    "\n",
    "* 0 = ***setosa***\n",
    "* 1 = ***versicolor***\n",
    "* 2 = ***virginica***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation graphique des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot_data(dataset)\n",
    "features_distributions(dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prétraitement"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le but est de nettoyer/optimiser les données, de sorte à les rendre plus qualitatives, et/ou cohérentes, pour simplifier l'apprentissage machine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La fonction MinMaxScaler permet de normaliser les données \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Fonction retirant les lignes dupliquées\n",
    "from check_data import clean_duplicated"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duplications"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On retirer les lignes présentes en double, si elles existent :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrait des lignes dupliquées\n",
    "dataset = clean_duplicated(dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalisation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On normalise ici les données d'entrainement :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resplitter le dataset en deux\n",
    "x_train, y_train = dataset.iloc[:, 0:4], dataset.iloc[:, 4:]\n",
    "\n",
    "# Créer un objet MinMaxScaler\n",
    "normalizer = MinMaxScaler()\n",
    "\n",
    "# Normaliser les variables\n",
    "x_train = normalizer.fit_transform(x_train)\n",
    "\n",
    "# Retransformer x_train en dataframe\n",
    "x_train = pd.DataFrame(data=x_train, columns=normalizer.get_feature_names_out())\n",
    "\n",
    "# Afficher les résultats de la normalisation\n",
    "display(x_train.head(), y_train.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On normalise ici les données données de test :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le test_set\n",
    "test_set = pd.read_csv(\"test_set.csv\")\n",
    "\n",
    "# Séparer les données en deux sets\n",
    "x_test, y_test = test_set.iloc[:, 0:4], test_set.iloc[:, 4:]\n",
    "\n",
    "# Normaliser le test_set\n",
    "x_test = normalizer.transform(x_test)\n",
    "\n",
    "# Retransformer x_test en dataframe\n",
    "x_test = pd.DataFrame(data=x_test, columns=normalizer.get_feature_names_out())\n",
    "\n",
    "# Afficher le test_set\n",
    "display(x_test.head(), y_test.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entraînement"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On cherche à déterminer quel est le meilleur algorithme à utiliser. Pour répondre à cette question, nous importons différents algorithmes candidats.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plusieurs algorithmes d'apprentissage que nous allons tester\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from eval_model import plot_ResTrainning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour les départager, on testera 1 à 1 les différents algorithmes (par utilisation d'une boucle *for*), en s'assurant de la reproductibilité des résultats :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mettre les algorithmes dans un liste pour les itérer\n",
    "learning_algo = [MultinomialNB(), LogisticRegression(random_state=42), SVC(random_state=42), DecisionTreeClassifier(random_state=42), KNeighborsClassifier()]\n",
    "\n",
    "# Pour chaque algorithme\n",
    "for algo in learning_algo :\n",
    "\n",
    "    # Entraîner le modèle (instance de l'algorithme)\n",
    "    tmp = algo\n",
    "    tmp.fit(x_train, y_train.values.ravel())\n",
    "\n",
    "    # Afficher les résultats\n",
    "    plot_ResTrainning(tmp, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On trouve que le meilleur modèle, pour ces données, est le ***DecisionTreeClassifier***."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimisation du modèle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va, à présent, chercher les meilleurs hyperparamètres pour le ***DecisionTreeClassifier***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permet la recherche des meilleurs hyperparamètres\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "\n",
    "# Pour l'affichage graphique\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pour afficher l'arbre de décision \n",
    "from sklearn.tree import plot_tree "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On définit les paramètres de la recherche : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instancer un arbre\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "# Définir les paramètres à tester\n",
    "params = {\"max_depth\" : [2, 3, 5, 10, 20, None],\n",
    "         \"min_samples_leaf\" : [2, 5, 10, 20, 50, 100],\n",
    "         \"criterion\" : [\"gini\", \"entropy\"],\n",
    "         \"random_state\" : [i for i in range(0, 43, 1)]}\n",
    "\n",
    "# Instancer l'itérateur pour la création du set de validation durant la recherche des meilleurs hyperparamètres\n",
    "kfold = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# Créer la grille de recherche\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=params, scoring=\"accuracy\", cv=kfold, verbose=3)\n",
    "\n",
    "# Effectuer la recherche + Afficher les résultats\n",
    "grid_search.fit(x_train, y_train.values.ravel())\n",
    "print(\"\\nles meilleurs paramètres du modèle sont : {}\".format(grid_search.best_params_))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On évalue le modèle optimisé :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupérer le meilleur modèle\n",
    "model = grid_search.best_estimator_\n",
    "\n",
    "plot_ResTrainning(model, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On analyse ici le fonctionnement du modèle, dans sa démarche de classification :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage de l'arbre de décision\n",
    "plt.figure(figsize=(14, 10))\n",
    "_ = plot_tree(model, feature_names=x_test.columns, class_names=[\"Setosa\", \"Versicolor\", \"Virginica\"], filled=True)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sauvegarde du modèle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fois toutes les étapes effectuer, on doit sauvegarder à la fois la chaîne de prétraitement, ainsi que le modèle entrainé, pour de futurs classifications.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permet de sauvegarder des objets python\n",
    "import pickle as pk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instancer le modèle avec les meilleurs hyperparamètres\n",
    "model = DecisionTreeClassifier(criterion=\"entropy\", max_depth=5, min_samples_leaf=5, random_state=0)\n",
    "\n",
    "# Entraîner le modèle sur l'ensemble des données\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# Sauvegarder le préprocesseur, il sera utiliser sur les futurs données\n",
    "pk.dump(normalizer, open(\"preprocesseur.sav\", 'wb'))\n",
    "\n",
    "# Sauvegarder le modèle\n",
    "pk.dump(model, open(\"model.sav\", 'wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pour allez plus loin"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation :\n",
    "- [matplotlib](https://matplotlib.org/stable/index.html)\n",
    "- [numpy](https://numpy.org/doc/)\n",
    "- [pandas](https://pandas.pydata.org/docs/)\n",
    "- [seaborn](https://seaborn.pydata.org/)\n",
    "- [sklearn](https://scikit-learn.org/stable/)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sites (liste non-exhaustive) :\n",
    "- [kaggle](https://kaggle.com)\n",
    "- [Machine Learning Mastery](https://machinelearningmastery.com/)\n",
    "- [toward data science](https://towardsdatascience.com/)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chaînes Youtubes (liste non-exhaustive) :\n",
    "- [AIforyou - Morgan Gautherot](https://www.youtube.com/@AIforyouMorganGautherot) (FR)\n",
    "- [Alexander Amini](https://www.youtube.com/@AAmini/videos) (ENG)\n",
    "- [Machine learnia](https://www.youtube.com/@MachineLearnia) (FR)\n",
    "- [Science4All](https://www.youtube.com/@Science4Allfrancais) (FR)\n",
    "- [StatQuest with Josh Starmer](https://www.youtube.com/@statquest) (ENG)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "66168a667a11ac16aca0d0d9742c8419f93457ff66115bf85239ea370d417636"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
